# 统计，机器学习，深度学习


**期望，均值，方差和样本方差**

期望和方差是面向总体样本空间，而样本的均值和方差是样本的统计量。其中均值是以
“上帝视角”角度表征总体的均值，类似的方差通常表示总体样本的方差。它们分别用
$\mu$和$\sigma^{2}$表示，且通常是未知的（一般情况下无法捕捉总体的样本空间）。

我们通常说的均值是指观察到的特定样本的均值，比如抽样$n$个样本$X_1, X_2, ..., X_n$
($n$个随机变量)，
样本均值表示为:

$$\overline{X} = \frac{1}{n}\sum_{i=1}^nX_i$$
显然

$$E(\overline{X}) = \mu$$
均值的方差为（证明可以看[这里](https://math.stackexchange.com/questions/1363505/prove-that-e-overlinex-mu2-frac1n-sigma2?answertab=active#tab-top))
$$Var(\overline{X}) = \frac{1}{n}\sigma^2$$

因为$\overline{X}$的期望为$\mu$,所以$\overline{X}$可以看做是期望（总体均值）
$\mu$的无偏估计。

样本方差$S_n^2$用于近似表示总体方差$\sigma^2$:

$$S_n^2 = \frac{1}{n-1}\sum_{i=1}^n(X_i - \mu)^2$$
注意分母是$n-1$，而不是我们直觉的$n$。具体证明可参见
[这里](https://www.zhihu.com/question/20099757), 简单来说
首先对于任意的随机样本有:

$$E[(X_i - \mu)^2] = \sigma^2$$
即有

$$E[\frac{1}{n}\sum_{i=1}^n(X_i - \mu)^2] = \sigma^2$$

而

$$\frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^2 = \frac{1}{n}\sum_{i=1}^n(X_i - \ 
\mu)^2 - (\mu - \overline{X})^2$$


因此如果直接使用$\frac{1}{n}\sum_{i=1}^n(X_i - \overline{X})^2$显然小于总体方差，
因此为了更准确的对总体方差进行无偏估计，把分母换成$n - 1$，作为对总体方差的无偏
估计。可以证明:

$$E[\frac{1}{n-1}\sum_{i=1}^n(X_i - \overline{X})^2] = \
E[\frac{1}{n}\sum_{i=1}^n(X_i - \mu)^2] = \sigma^2 = \sigma^2$$


**什么是效应量 effect size**

  p值通常用于判定差异是否显著。一般我们认为 p 小于 0.05，则差异显著，那么差别
  到底有多大，效应量是对差异程度的度量。也就是说 p 用于定性是否显著差异，效应量
  用于定量差异有多大。效应量有很多种，参考
  [这里](https://www.psychometrica.de/effect_size.html)，其中最常用的是
  $d_{Cohen}$, 均值与两组数据整合在一起的标准差的比值。

**第一类和第二类错误**

  两类错误都是针对假设检验中的 H0 所说的。第一类错误, 实际 H0 正确,结果却拒绝 
  H0, 称为拒真,用$\alpha$表示; 第二类错误, 实际 H0 错误, 结果却接受 H0，称为取伪
  用$\beta$表示。
  
  当样本数固定时,$\alpha$越大,$\beta$就越小, 只有增大样本量,才可使两者同时减小,
  统计中$\alpha$通常作为检验水准(level of a test), $1 - \beta$作为检验效能
  (power of a test). 参考[这里](https://www.zhihu.com/question/20993864)和
  [这里](https://zhuanlan.zhihu.com/p/146559807).
  
  